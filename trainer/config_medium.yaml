# Main configuration file for chess neural network training
# Optimized configuration for large-scale training with 1M positions, includes all features

model:
  # Neural network architecture parameters
  d_model: 256  # Model dimension (embedding size) - should be power of 2
  nhead: 8      # Number of attention heads - must divide d_model
  num_layers: 6 # Number of transformer encoder layers
  dim_feedforward: 1024 # Feedforward network dimension
  dropout: 0.1  # Dropout rate (0.0 to 1.0)
  input_channels: 18 # Number of input feature planes (8x8x18 board representation)

training:
  # Training hyperparameters
  batch_size: 512                    # Training batch size (increased for better performance)
  learning_rate: 0.0001              # Initial learning rate (10x higher for large dataset)
  weight_decay: 0.00001             # L2 regularization strength
  num_epochs: 1                   # Maximum number of training epochs (increased for longer training)
  save_every: 5                     # Save checkpoint every N epochs (more frequent saves)
  checkpoint_every_batches: 1000    # Save checkpoint and run validation every N batches
  print_every_batches: 1000        # Print training progress every N batches
  early_stopping_patience: 15      # Stop training if no improvement for N epochs (increased patience)
  optimizer: "adamw"               # Optimizer type: 'adam', 'adamw', or 'sgd'
  betas: [0.9, 0.999]              # Beta parameters for Adam optimizer
  gradient_clip_norm: 1.0          # Maximum gradient norm for clipping

data:
  # Data loading parameters
  train_file: "trainer/localdata/lichess_db_eval.train.jsonl.zst"  # Path to the training data file
  val_file: "trainer/localdata/lichess_db_eval.val.jsonl.zst"    # Path to the validation data file (small, cached)
  num_workers: 0                    # Number of worker processes for data loading (must be 0 for streaming)
  pin_memory: true                  # Pin memory for faster GPU transfer
  max_positions: 100000000            # Maximum number of positions to load (1M for large-scale training)
  # shuffle: removed - input data is already shuffled
  buffer_batches: 20               # Number of batches to read in one go (buffer_size = batch_size Ã— buffer_batches)
  cp_to_prob_scale: 20.0          # Scale factor for centipawn to probability conversion (400=moderate, 200=steeper, 600=gentler)

hardware:
  # Hardware configuration
  device: "auto"                    # Device to use: 'auto', 'cpu', 'cuda', or 'mps'
  mixed_precision: false            # Use mixed precision training (requires GPU)

paths:
  # File paths (set these when using the config)
  data_file: "trainer/localdata/lichess_db_eval.jsonl.zst" # Path to training data file (.zst format)
  save_dir: "trainer/checkpoints"           # Directory to save model checkpoints
  log_dir: "trainer/logs"                   # Directory to save training logs
